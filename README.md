# Zephyr Platform

## Descri√ß√£o

O **Zephyr Platform** √© um projeto focado em comparar duas abordagens para consumir e persistir dados de um arquivo `.csv`: utilizando um servi√ßo consumidor tradicional e usando o **Kafka Connect**.

Este reposit√≥rio serve como um "pai" para os m√≥dulos principais do projeto, organizando os reposit√≥rios individuais:
- **[Zephyr Producer](https://github.com/marcuslira2/zephyr-producer)** - Respons√°vel por ler arquivos CSV e produzir mensagens para o Kafka.
- **[Zephyr Consumer](https://github.com/marcuslira2/zephyr-consumer)** - Respons√°vel por consumir as mensagens do Kafka e persistir no banco de dados.
- **[Zephyr Infra](https://github.com/marcuslira2/zephyr-infra)** - Cont√©m a infraestrutura Docker para subir o ambiente com Kafka, PostgreSQL e Kafka Connect.

---

## üìå Infraestrutura e Servi√ßos

O `docker-compose.yml` localizado no reposit√≥rio **Zephyr Infra** define e configura um ambiente baseado em cont√™ineres com **Kafka**, **PostgreSQL** e **Kafka Connect**, facilitando a inicializa√ß√£o e a orquestra√ß√£o dos servi√ßos necess√°rios para o processamento de mensagens e persist√™ncia de dados.

### 1. PostgreSQL (`postgres`)
- Banco de dados relacional para armazenamento de dados.
- Persist√™ncia garantida atrav√©s do volume `pgdata`.
- Exposto na porta **5432**.

### 2. Zookeeper (`zookeeper`)
- Servi√ßo de coordena√ß√£o necess√°rio para o funcionamento do Kafka.
- Exposto na porta **2181**.

### 3. Kafka (`kafka`)
- Broker de mensagens usado para publicar e consumir eventos.
- Configurado para escutar nas portas **9092** (externa) e **9093** (interna para comunica√ß√£o entre cont√™ineres).
- Permite a cria√ß√£o autom√°tica de t√≥picos.

### 4. Kafka Connect (`kafka-connect`)
- Framework para integrar Kafka com bancos de dados e outros sistemas.
- Conectado ao broker Kafka (`kafka:9093`).
- Exp√µe sua API REST na porta **8083**.
- Configurado para armazenar informa√ß√µes de configura√ß√£o, offsets e status em t√≥picos internos.
- Plugins e conectores podem ser adicionados no diret√≥rio `./connect-plugins`.

---

## ‚öôÔ∏è Configura√ß√£o do Conector Sink (`sink-config.json`)

### PostgreSQL Sink Connector
Este projeto utiliza o **Kafka Connect** com o **JDBC Sink Connector** para armazenar dados do Kafka em um banco **PostgreSQL**.

### Configura√ß√£o do Conector:
- **T√≥pico de origem**: `topic.csv.kafka`
- **Banco de destino**: PostgreSQL (`kafkateste`)
- **Tabela**: `product` (criada automaticamente se n√£o existir)
- **Modo de inser√ß√£o**: `insert` (apenas insere novos registros)
- **Convers√£o de dados**: JSON

### üöÄ Como Registrar o Conector
1. Certifique-se de que o **Kafka Connect** est√° rodando.
2. Envie a configura√ß√£o via API REST do Kafka Connect:
   ```bash
   curl -X POST -H "Content-Type: application/json" --data @sink-config.json http://localhost:8083/connectors
   ```
3. Monitore o status do conector:
   ```bash
   curl -X GET http://localhost:8083/connectors/postgres-sink-connector/status
   ```

Esse conector facilita a ingest√£o cont√≠nua de dados do Kafka para o PostgreSQL, garantindo escalabilidade e compatibilidade com JSON. üöÄ

---

## ‚òï Servi√ßos Java

Os servi√ßos Java foram desenvolvidos com **Java 17**, utilizando **Maven** para automa√ß√£o de build.

- **Zephyr Producer**:
  - L√™ um arquivo CSV da pasta configurada.
  - Envia os dados para um t√≥pico correspondente ao tipo de processamento desejado:
    - **CONSUMER**: Envia um lote de mensagens comprimidas para que o servi√ßo consumidor receba, descomprima, leia e persista os dados.
    - **KAFKA**: Ajusta a mensagem conforme a configura√ß√£o do **Kafka Connect** e do `sink-config.json`, permitindo persist√™ncia direta no banco.

---

## üõ† Como Iniciar o Projeto

### üìå Pr√©-requisitos
- **Java 17**
- **Docker**
- **Maven**
- **Postman** (ou outra ferramenta para enviar requisi√ß√µes HTTP)

### üöÄ Passos para Configura√ß√£o
1. Clone os reposit√≥rios individuais e configure cada um conforme as instru√ß√µes em seus respectivos `README.md`.
2. Configure o ambiente do Java corretamente.
3. Ajuste o arquivo `sink-config.json` conforme necess√°rio.
4. Configure os arquivos `application.properties` ou `application.yml` dos servi√ßos Java.
5. Importe a collection de requisi√ß√µes que est√° na pasta `config` do Postman.
6. Suba os dois servi√ßos Java (produtor e consumidor).
7. Envie as requisi√ß√µes via Postman para processar os arquivos CSV.

---

## üîç Comandos √öteis para An√°lise

### üìä Verificar se um t√≥pico tem mensagens acumuladas:
```bash
docker exec -it kafka kafka-run-class kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic {NOME_DO_TOPICO} --time -1
```

### üìú Listar t√≥picos dispon√≠veis:
```bash
docker exec -it kafka kafka-topics --list --bootstrap-server localhost:9092
```

### üì• Ler uma mensagem do t√≥pico:
```bash
docker exec -it kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic {NOME_DO_TOPICO} --from-beginning --max-messages 1 --property print.value=true
```

---

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa **MIT**. Sinta-se livre para usar, modificar e contribuir!

---

## üì´ Contato
Caso tenha alguma d√∫vida ou sugest√£o, entre em contato via [GitHub Issues](https://github.com/marcuslira2/zephyr-platform/issues).

